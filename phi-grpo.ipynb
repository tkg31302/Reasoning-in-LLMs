{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b9230e976b34a9ea85978cf22857012":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f8ead1775934dc3a10533b67b3dd905":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129dc789722b43439574390bba63b36a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b9f8a2a793640d689abc10f5f39c54b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22e0933485c14d94b0c1cfe198d6758f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43462d5de24b4e55871b3f579798b374","IPY_MODEL_99577e7cbed74c89afb3d44d4fd956c5","IPY_MODEL_d034c840e7f74177a7b07a188d666b8d"],"layout":"IPY_MODEL_0f8ead1775934dc3a10533b67b3dd905"}},"4089236deafd4fa2be86d8dc0a29d469":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43462d5de24b4e55871b3f579798b374":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e43ad27d5d304d1ebf9b374016409a97","placeholder":"​","style":"IPY_MODEL_51948945111f437c9ed6ccab22072dd3","value":""}},"47d2fd7f76754d9fa156576bc0c58abb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c9248100f89400d9e1407dbb168d5d6","placeholder":"​","style":"IPY_MODEL_96cea0d773c8426b8be72dd7f72e5a82","value":""}},"4c9248100f89400d9e1407dbb168d5d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51948945111f437c9ed6ccab22072dd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"627f68389cf64e2a915a72ab147ee8a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78cc90a50c0c4636b0f41436a820ecd3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81a0791760de4dcebd543c40d2c1e322":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b9f8a2a793640d689abc10f5f39c54b","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_627f68389cf64e2a915a72ab147ee8a7","value":2}},"8991360910ef417db03499f76f5fe323":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96cea0d773c8426b8be72dd7f72e5a82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99577e7cbed74c89afb3d44d4fd956c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4089236deafd4fa2be86d8dc0a29d469","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78cc90a50c0c4636b0f41436a820ecd3","value":2}},"9eed940f3815428583b4ddefc1a81469":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a729c5fc5c764c85885cac7a2d4d95d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9eed940f3815428583b4ddefc1a81469","placeholder":"​","style":"IPY_MODEL_0b9230e976b34a9ea85978cf22857012","value":"Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:51&lt;00:00, 25.46s/it]\n"}},"d034c840e7f74177a7b07a188d666b8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_129dc789722b43439574390bba63b36a","placeholder":"​","style":"IPY_MODEL_8991360910ef417db03499f76f5fe323","value":"Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:47&lt;00:00, 23.63s/it]\n"}},"d6d5a7d96a034247b38d25d8a9cc979c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e43ad27d5d304d1ebf9b374016409a97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f57d844b2efa469e8aadd48175ce70ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47d2fd7f76754d9fa156576bc0c58abb","IPY_MODEL_81a0791760de4dcebd543c40d2c1e322","IPY_MODEL_a729c5fc5c764c85885cac7a2d4d95d0"],"layout":"IPY_MODEL_d6d5a7d96a034247b38d25d8a9cc979c"}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installation","metadata":{}},{"cell_type":"code","source":" %%capture\n!pip install unsloth vllm\n!pip install triton==3.1.0\n!pip install -U pynvml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:36:36.497888Z","iopub.execute_input":"2025-03-08T02:36:36.498201Z","iopub.status.idle":"2025-03-08T02:37:52.377549Z","shell.execute_reply.started":"2025-03-08T02:36:36.498170Z","shell.execute_reply":"2025-03-08T02:37:52.376268Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Unsloth","metadata":{}},{"cell_type":"markdown","source":"Use `PatchFastRL` before all functions to patch GRPO and other RL algorithms!","metadata":{"id":"XaBUw4wJ2WlP"}},{"cell_type":"code","source":"from unsloth import FastLanguageModel, PatchFastRL\nPatchFastRL(\"GRPO\", FastLanguageModel)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59DIs5BMcvjN","outputId":"5d03d8e6-d961-4101-8cd5-669944f91a5e","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:37:52.378717Z","iopub.execute_input":"2025-03-08T02:37:52.379086Z","iopub.status.idle":"2025-03-08T02:38:27.338902Z","shell.execute_reply.started":"2025-03-08T02:37:52.379047Z","shell.execute_reply":"2025-03-08T02:38:27.337905Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Patching Xformers to fix some performance issues.\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Load up `Phi-4 14B`, and set parameters","metadata":{"id":"KROEggs82dUA"}},{"cell_type":"code","source":"hp_model_name = \"unsloth/Phi-4\"\nhp_dataset = 'gsm'\nhp_max_steps = 250\nhp_run_name = f\"{hp_model_name.split('/')[1]}-{hp_dataset}-{hp_max_steps}-steps\"\n\n\nhp_lora_rank = 16\nhp_max_comp_len = 200","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:38:27.339742Z","iopub.execute_input":"2025-03-08T02:38:27.339986Z","iopub.status.idle":"2025-03-08T02:38:27.346199Z","shell.execute_reply.started":"2025-03-08T02:38:27.339964Z","shell.execute_reply":"2025-03-08T02:38:27.344954Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from unsloth import is_bfloat16_supported\nimport torch\nmax_seq_length = 512 # Can increase for longer reasoning traces\nlora_rank = 16 # Larger rank = smarter, but slower\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = hp_model_name,\n    max_seq_length = max_seq_length,\n    load_in_4bit = True, # False for LoRA 16bit\n    fast_inference = True, # Enable vLLM fast inference\n    max_lora_rank = lora_rank,\n    gpu_memory_utilization = 0.7, # Reduce if out of memory\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = lora_rank,\n    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n    random_state = 3407,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":805,"referenced_widgets":["f57d844b2efa469e8aadd48175ce70ab","47d2fd7f76754d9fa156576bc0c58abb","81a0791760de4dcebd543c40d2c1e322","a729c5fc5c764c85885cac7a2d4d95d0","d6d5a7d96a034247b38d25d8a9cc979c","4c9248100f89400d9e1407dbb168d5d6","96cea0d773c8426b8be72dd7f72e5a82","1b9f8a2a793640d689abc10f5f39c54b","627f68389cf64e2a915a72ab147ee8a7","9eed940f3815428583b4ddefc1a81469","0b9230e976b34a9ea85978cf22857012","22e0933485c14d94b0c1cfe198d6758f","43462d5de24b4e55871b3f579798b374","99577e7cbed74c89afb3d44d4fd956c5","d034c840e7f74177a7b07a188d666b8d","0f8ead1775934dc3a10533b67b3dd905","e43ad27d5d304d1ebf9b374016409a97","51948945111f437c9ed6ccab22072dd3","4089236deafd4fa2be86d8dc0a29d469","78cc90a50c0c4636b0f41436a820ecd3","129dc789722b43439574390bba63b36a","8991360910ef417db03499f76f5fe323"]},"id":"DkIvEkIIkEyB","outputId":"c5a32856-2166-4485-fdb3-16241d0e6316","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:38:27.348739Z","iopub.execute_input":"2025-03-08T02:38:27.348996Z","iopub.status.idle":"2025-03-08T02:40:43.249205Z","shell.execute_reply.started":"2025-03-08T02:38:27.348976Z","shell.execute_reply":"2025-03-08T02:40:43.248513Z"}},"outputs":[{"name":"stdout","text":"INFO 03-08 02:38:28 __init__.py:207] Automatically detected platform cuda.\nUnsloth: Switching from Unsloth dynamic quant to normal quant since\nwe do not yet support fast inference for unsloth/phi-4-unsloth-bnb-4bit\n==((====))==  Unsloth 2025.3.8: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d24f5db3b64e16839bf1b18913020b"}},"metadata":{}},{"name":"stdout","text":"Unsloth: vLLM loading unsloth/phi-4-bnb-4bit with actual GPU utilization = 69.34%\nUnsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\nUnsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 128.\nUnsloth: vLLM's KV Cache can use up to 0.25 GB. Also swap space = 5 GB.\nWARNING 03-08 02:38:29 config.py:2448] Casting torch.bfloat16 to torch.float16.\nINFO 03-08 02:38:42 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\nUnsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\nINFO 03-08 02:38:42 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/phi-4-bnb-4bit', speculative_config=None, tokenizer='unsloth/phi-4-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/phi-4-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":128}, use_cached_outputs=False, \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/18.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4520e17364c2420abb99cfc3f93d4729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d19f7c210c4d71a856dffff0d5ecbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2f0cb1ed9e410dbb62821e43be58bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba87f95f39346e6af4e3a6db84731b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f3416418ee4ba7a4e7d7230e1dd730"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/170 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aae9a20eb4cf41e7ae7847543d6a977d"}},"metadata":{}},{"name":"stdout","text":"INFO 03-08 02:38:44 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-08 02:38:44 cuda.py:226] Using XFormers backend.\nINFO 03-08 02:38:54 model_runner.py:1110] Starting to load model unsloth/phi-4-bnb-4bit...\nINFO 03-08 02:38:55 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\nINFO 03-08 02:38:55 weight_utils.py:254] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b180da39f0045f7b0c5b73c2aad8c5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcdb0642c01b45459e9b5b20485d8256"}},"metadata":{}},{"name":"stdout","text":"INFO 03-08 02:39:20 weight_utils.py:270] Time spent downloading weights for unsloth/phi-4-bnb-4bit: 25.263081 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"effdea5b70814429a5887c2ae311bdfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14bcc0a42bfd45dea8841eb5cec9e35f"}},"metadata":{}},{"name":"stdout","text":"INFO 03-08 02:39:29 model_runner.py:1115] Loading model weights took 8.4920 GB\nINFO 03-08 02:39:29 logger.py:57] Using PunicaWrapperGPU.\nINFO 03-08 02:39:43 worker.py:267] Memory profiling takes 14.09 seconds\nINFO 03-08 02:39:43 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.69) = 10.22GiB\nINFO 03-08 02:39:43 worker.py:267] model weights take 8.49GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 1.23GiB.\nINFO 03-08 02:39:44 executor_base.py:111] # cuda blocks: 403, # CPU blocks: 1638\nINFO 03-08 02:39:44 executor_base.py:116] Maximum concurrency for 512 tokens per request: 12.59x\nINFO 03-08 02:39:48 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"name":"stderr","text":"Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:40<00:00,  2.14s/it]","output_type":"stream"},{"name":"stdout","text":"INFO 03-08 02:40:29 model_runner.py:1562] Graph capturing finished in 41 secs, took 0.65 GiB\nINFO 03-08 02:40:29 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 59.98 seconds\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/18.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0c0057ce474d5584e62e2ed68d1f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05740878fc9143538b79e23b8e61fc4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de203bd17ab04764a3a4abca1b62d6f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012c12d832d646c59d7c52bbc03d4242"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0f3accb36e43a48c7d131f1ccf40b2"}},"metadata":{}},{"name":"stderr","text":"Not an error, but Unsloth cannot patch Attention layers with our manual autograd engine since either LoRA adapters\nare not enabled or a bias term (like in Qwen) is used.\nNot an error, but Unsloth cannot patch O projection layer with our manual autograd engine since either LoRA adapters\nare not enabled or a bias term (like in Qwen) is used.\nUnsloth 2025.3.8 patched 40 layers with 0 QKV layers, 0 O layers and 40 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Data Prep\n<a name=\"Data\"></a>\n\nWe directly leverage [@willccbb](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb) for data prep and all reward functions. You are free to create your own!","metadata":{"id":"SSlkA49z2xZB"}},{"cell_type":"code","source":"import re\nfrom datasets import load_dataset, Dataset\n\n# Load and prep dataset\nSYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\"\"\"\n\nXML_COT_FORMAT = \"\"\"\\\n<reasoning>\n{reasoning}\n</reasoning>\n<answer>\n{answer}\n</answer>\n\"\"\"\n\ndef extract_xml_answer(text: str) -> str:\n    answer = text.split(\"<answer>\")[-1]\n    answer = answer.split(\"</answer>\")[0]\n    return answer.strip()\n\ndef extract_hash_answer(text: str) -> str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n# uncomment middle messages for 1-shot prompting\ndef get_gsm8k_questions(split=\"train\") -> Dataset:\n    data = load_dataset('openai/gsm8k', 'main')[split]  # type: ignore\n    data = data.map(lambda x: {  # type: ignore\n        'prompt': tokenizer.apply_chat_template([\n            {'role': 'system', 'content': SYSTEM_PROMPT},\n            {'role': 'user', 'content': x['question']}\n            # {'role': 'user', 'content': SYSTEM_PROMPT + '\\n' + x['question']}\n        ], tokenize=False, continue_final_message=True),\n        'question': x['question'],\n        'answer': extract_hash_answer(x['answer'])\n    })  # type: ignore\n    return data  # type: ignore\n    \ndataset = get_gsm8k_questions()\n\n# Reward functions\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    # responses = [completion[0]['content'] for completion in completions]\n    # q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in completions]\n    # print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n\ndef int_reward_func(completions, **kwargs) -> list[float]:\n    extracted_responses = [extract_xml_answer(r) for r in completions]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    matches = [re.match(pattern, r) for r in completions]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef soft_format_reward_func(completions, **kwargs) -> list[float]:\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n    matches = [re.match(pattern, r) for r in completions]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef count_xml(text) -> float:\n    count = 0.0\n    if text.count(\"<reasoning>\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n</reasoning>\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n<answer>\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n    if text.count(\"\\n</answer>\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -> list[float]:\n    return [count_xml(c) for c in completions]","metadata":{"id":"cXk993X6C2ZZ","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:40:43.252874Z","iopub.execute_input":"2025-03-08T02:40:43.253137Z","iopub.status.idle":"2025-03-08T02:40:45.687714Z","shell.execute_reply.started":"2025-03-08T02:40:43.253111Z","shell.execute_reply":"2025-03-08T02:40:45.686759Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b1998717d049b98156c0ea7e1b3738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556ad2f23a4742bcb213dd28836eddea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ed3b5253294684bddb991c9eaeb738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0ccfacdde0453db5a72cdb945d3274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f3633e4d194bedbea7aedaf55b5da5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625622eb44284791b912f400b4b990f0"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"<a name=\"Train\"></a>\n### Train the model\n\nNow set up GRPO Trainer and all configurations!","metadata":{"id":"Tze5NF5523DB"}},{"cell_type":"code","source":"from trl import GRPOConfig, GRPOTrainer\ntraining_args = GRPOConfig(\n    use_vllm = True, # use vLLM for fast inference!\n    learning_rate = 5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type = \"cosine\",\n    optim = \"paged_adamw_8bit\",\n    logging_steps = 1,\n    bf16 = is_bfloat16_supported(),\n    fp16 = not is_bfloat16_supported(),\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n    num_generations = 6, # Decrease if out of memory\n    max_prompt_length = 256,\n    max_completion_length = hp_max_comp_len,\n    # num_train_epochs = 1, # Set to 1 for a full training run\n    max_steps = hp_max_steps,\n    save_steps = 250,\n    max_grad_norm = 0.1,\n    report_to = \"wandb\", # Can use Weights & Biases\n    output_dir = hp_run_name + '_out',\n    run_name = hp_run_name\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptqkXK2D4d6p","outputId":"344b54e8-5a9c-4676-bfc0-23f8b5cb7426","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:40:45.688652Z","iopub.execute_input":"2025-03-08T02:40:45.688998Z","iopub.status.idle":"2025-03-08T02:40:45.729547Z","shell.execute_reply.started":"2025-03-08T02:40:45.688966Z","shell.execute_reply":"2025-03-08T02:40:45.728774Z"}},"outputs":[{"name":"stdout","text":"Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\nWe will change the batch size of 1 to the `num_generations` of 6\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n\nYou might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n\n| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n|------|---------------|-----------|------------|-------------------|----------|\n| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n","metadata":{"id":"n8egDqHG3GH0"}},{"cell_type":"code","source":"import wandb\nimport os\n\nwandb.login(key='86ba02d4ce42eb3527e4f33a4bd8e46b4bcbfce2')\nos.environ[\"WANDB_PROJECT\"] = \"dsc_250_train\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:40:45.730381Z","iopub.execute_input":"2025-03-08T02:40:45.730655Z","iopub.status.idle":"2025-03-08T02:40:52.094826Z","shell.execute_reply.started":"2025-03-08T02:40:45.730621Z","shell.execute_reply":"2025-03-08T02:40:52.093862Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikhil-23\u001b[0m (\u001b[33mteam-nik\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"trainer = GRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"vzOuSVCL_GA9","outputId":"0fe20ec2-ea69-486a-e2df-4685bd390413","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T02:40:52.095719Z","iopub.execute_input":"2025-03-08T02:40:52.096417Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 250\nO^O/ \\_/ \\    Batch size per device = 12 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (12 x 1 x 1) = 12\n \"-____-\"     Trainable parameters = 44,236,800/7,888,000,000 (0.56% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250308_024054-vaby0snz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/team-nik/dsc_250_train/runs/vaby0snz' target=\"_blank\">Phi-4-gsm-250-steps</a></strong> to <a href='https://wandb.ai/team-nik/dsc_250_train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/team-nik/dsc_250_train' target=\"_blank\">https://wandb.ai/team-nik/dsc_250_train</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/team-nik/dsc_250_train/runs/vaby0snz' target=\"_blank\">https://wandb.ai/team-nik/dsc_250_train/runs/vaby0snz</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='110' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [110/250 3:58:25 < 5:09:04, 0.01 it/s, Epoch 0.03/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>reward</th>\n      <th>reward_std</th>\n      <th>completion_length</th>\n      <th>kl</th>\n      <th>rewards / xmlcount_reward_func</th>\n      <th>rewards / soft_format_reward_func</th>\n      <th>rewards / strict_format_reward_func</th>\n      <th>rewards / int_reward_func</th>\n      <th>rewards / correctness_reward_func</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>140.166672</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.032275</td>\n      <td>103.333336</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>114.416672</td>\n      <td>0.000011</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>-0.040833</td>\n      <td>0.183327</td>\n      <td>125.250000</td>\n      <td>0.000008</td>\n      <td>-0.040833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>95.583336</td>\n      <td>0.000009</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000000</td>\n      <td>0.233417</td>\n      <td>0.492777</td>\n      <td>135.416672</td>\n      <td>0.000006</td>\n      <td>0.025083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000000</td>\n      <td>-0.052333</td>\n      <td>0.128190</td>\n      <td>100.583336</td>\n      <td>0.000008</td>\n      <td>-0.052333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000000</td>\n      <td>-0.034333</td>\n      <td>0.167405</td>\n      <td>94.250000</td>\n      <td>0.000006</td>\n      <td>-0.034333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>99.416672</td>\n      <td>0.000007</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>90.833336</td>\n      <td>0.000007</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>103.583336</td>\n      <td>0.000012</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.051031</td>\n      <td>88.916672</td>\n      <td>0.000011</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>95.583336</td>\n      <td>0.000016</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.000000</td>\n      <td>-0.030833</td>\n      <td>0.159528</td>\n      <td>139.250000</td>\n      <td>0.000012</td>\n      <td>-0.030833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>133.333344</td>\n      <td>0.000007</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>108.833336</td>\n      <td>0.000014</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>126.583336</td>\n      <td>0.000007</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.000000</td>\n      <td>0.005667</td>\n      <td>0.047618</td>\n      <td>73.833336</td>\n      <td>0.000008</td>\n      <td>0.005667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.000000</td>\n      <td>0.032917</td>\n      <td>0.080629</td>\n      <td>83.166672</td>\n      <td>0.000009</td>\n      <td>0.032917</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>89.416672</td>\n      <td>0.000010</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>70.250000</td>\n      <td>0.000011</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>99.583336</td>\n      <td>0.000010</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>77.333336</td>\n      <td>0.000020</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.000000</td>\n      <td>-0.014250</td>\n      <td>0.034905</td>\n      <td>64.583336</td>\n      <td>0.000011</td>\n      <td>-0.014250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>108.166672</td>\n      <td>0.000014</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.000000</td>\n      <td>0.011083</td>\n      <td>0.074914</td>\n      <td>81.333336</td>\n      <td>0.000017</td>\n      <td>0.011083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.000000</td>\n      <td>0.031250</td>\n      <td>0.057790</td>\n      <td>102.833336</td>\n      <td>0.000016</td>\n      <td>0.031250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.000000</td>\n      <td>-0.043500</td>\n      <td>0.139431</td>\n      <td>105.750000</td>\n      <td>0.000017</td>\n      <td>-0.043500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>128.250000</td>\n      <td>0.000021</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.000000</td>\n      <td>-0.008667</td>\n      <td>0.060559</td>\n      <td>110.416672</td>\n      <td>0.000027</td>\n      <td>-0.008667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>101.416672</td>\n      <td>0.000028</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>93.916672</td>\n      <td>0.000035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.000000</td>\n      <td>0.031250</td>\n      <td>0.057790</td>\n      <td>94.833336</td>\n      <td>0.000046</td>\n      <td>0.031250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>92.166672</td>\n      <td>0.000065</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.059748</td>\n      <td>125.166672</td>\n      <td>0.000047</td>\n      <td>0.041667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.000000</td>\n      <td>-0.023083</td>\n      <td>0.056542</td>\n      <td>72.833336</td>\n      <td>0.000742</td>\n      <td>-0.023083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>72.833336</td>\n      <td>0.000096</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>68.833336</td>\n      <td>0.000037</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>97.583336</td>\n      <td>0.000037</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.000000</td>\n      <td>0.019333</td>\n      <td>0.030086</td>\n      <td>133.833344</td>\n      <td>0.000043</td>\n      <td>0.019333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.000000</td>\n      <td>-0.037583</td>\n      <td>0.175804</td>\n      <td>105.583336</td>\n      <td>0.000031</td>\n      <td>-0.037583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>96.833336</td>\n      <td>0.000069</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.000000</td>\n      <td>-0.001583</td>\n      <td>0.042603</td>\n      <td>132.250000</td>\n      <td>0.000080</td>\n      <td>-0.001583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>102.083336</td>\n      <td>0.000034</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>102.000000</td>\n      <td>0.000064</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>97.833336</td>\n      <td>0.000135</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.000000</td>\n      <td>0.022167</td>\n      <td>0.034434</td>\n      <td>120.000000</td>\n      <td>0.000094</td>\n      <td>0.022167</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.000000</td>\n      <td>-0.015833</td>\n      <td>0.038784</td>\n      <td>83.250000</td>\n      <td>0.000130</td>\n      <td>-0.015833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>113.000000</td>\n      <td>0.000094</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.000000</td>\n      <td>0.026250</td>\n      <td>0.064299</td>\n      <td>112.083336</td>\n      <td>0.000070</td>\n      <td>0.026250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.000000</td>\n      <td>0.035250</td>\n      <td>0.061077</td>\n      <td>110.083336</td>\n      <td>0.000077</td>\n      <td>0.035250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.000000</td>\n      <td>0.022250</td>\n      <td>0.054501</td>\n      <td>75.000000</td>\n      <td>0.000255</td>\n      <td>0.022250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.051031</td>\n      <td>124.083336</td>\n      <td>0.000157</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>82.833336</td>\n      <td>0.000224</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.000000</td>\n      <td>0.023500</td>\n      <td>0.050135</td>\n      <td>85.750000</td>\n      <td>0.000180</td>\n      <td>0.023500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.032275</td>\n      <td>98.083336</td>\n      <td>0.000334</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.000000</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>107.583336</td>\n      <td>0.000200</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.000000</td>\n      <td>0.031250</td>\n      <td>0.057790</td>\n      <td>128.500000</td>\n      <td>0.000375</td>\n      <td>0.031250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.000000</td>\n      <td>0.023833</td>\n      <td>0.122158</td>\n      <td>139.916672</td>\n      <td>0.000333</td>\n      <td>0.023833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.000000</td>\n      <td>-0.106083</td>\n      <td>0.164350</td>\n      <td>147.750000</td>\n      <td>0.000322</td>\n      <td>-0.106083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.000000</td>\n      <td>0.036583</td>\n      <td>0.089611</td>\n      <td>87.250000</td>\n      <td>0.001132</td>\n      <td>0.036583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.051031</td>\n      <td>93.916672</td>\n      <td>0.000942</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.000000</td>\n      <td>-0.019083</td>\n      <td>0.154954</td>\n      <td>143.000000</td>\n      <td>0.000592</td>\n      <td>-0.019083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>131.583344</td>\n      <td>0.000633</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.000000</td>\n      <td>-0.064583</td>\n      <td>0.158196</td>\n      <td>117.583336</td>\n      <td>0.000659</td>\n      <td>-0.064583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.000000</td>\n      <td>-0.044750</td>\n      <td>0.160646</td>\n      <td>124.416672</td>\n      <td>0.001005</td>\n      <td>-0.044750</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.000100</td>\n      <td>0.010417</td>\n      <td>0.025516</td>\n      <td>98.083336</td>\n      <td>0.002685</td>\n      <td>0.010417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.000000</td>\n      <td>-0.065833</td>\n      <td>0.170591</td>\n      <td>148.416672</td>\n      <td>0.000999</td>\n      <td>-0.065833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.000100</td>\n      <td>0.056083</td>\n      <td>0.084678</td>\n      <td>107.833336</td>\n      <td>0.003747</td>\n      <td>0.056083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.000100</td>\n      <td>-0.025833</td>\n      <td>0.154303</td>\n      <td>121.833336</td>\n      <td>0.001739</td>\n      <td>-0.025833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.000000</td>\n      <td>0.020833</td>\n      <td>0.032275</td>\n      <td>146.416672</td>\n      <td>0.001197</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.000100</td>\n      <td>0.020833</td>\n      <td>0.032275</td>\n      <td>111.833336</td>\n      <td>0.003451</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.000100</td>\n      <td>0.011750</td>\n      <td>0.070113</td>\n      <td>111.416672</td>\n      <td>0.002212</td>\n      <td>0.011750</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.000100</td>\n      <td>0.031250</td>\n      <td>0.057790</td>\n      <td>136.583344</td>\n      <td>0.002949</td>\n      <td>0.031250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.001000</td>\n      <td>-0.032583</td>\n      <td>0.123676</td>\n      <td>115.166672</td>\n      <td>0.025459</td>\n      <td>-0.032583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.000100</td>\n      <td>0.020833</td>\n      <td>0.032275</td>\n      <td>131.333344</td>\n      <td>0.002221</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.000200</td>\n      <td>-0.011083</td>\n      <td>0.027149</td>\n      <td>125.500000</td>\n      <td>0.004123</td>\n      <td>-0.052750</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.000300</td>\n      <td>0.062500</td>\n      <td>0.068465</td>\n      <td>141.166672</td>\n      <td>0.006826</td>\n      <td>0.062500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.000200</td>\n      <td>0.058417</td>\n      <td>0.065035</td>\n      <td>100.333336</td>\n      <td>0.005726</td>\n      <td>0.058417</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.000100</td>\n      <td>-0.010250</td>\n      <td>0.173021</td>\n      <td>144.250000</td>\n      <td>0.002943</td>\n      <td>-0.010250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.000500</td>\n      <td>-0.023083</td>\n      <td>0.139848</td>\n      <td>148.750000</td>\n      <td>0.011460</td>\n      <td>-0.023083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.000300</td>\n      <td>0.083333</td>\n      <td>0.059748</td>\n      <td>169.833344</td>\n      <td>0.007229</td>\n      <td>0.083333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.000400</td>\n      <td>0.041667</td>\n      <td>0.059748</td>\n      <td>146.166672</td>\n      <td>0.009322</td>\n      <td>0.041667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.000400</td>\n      <td>0.052083</td>\n      <td>0.066508</td>\n      <td>150.750000</td>\n      <td>0.010387</td>\n      <td>0.052083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.000500</td>\n      <td>0.054917</td>\n      <td>0.065025</td>\n      <td>145.250000</td>\n      <td>0.011880</td>\n      <td>0.054917</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.000200</td>\n      <td>0.052083</td>\n      <td>0.057790</td>\n      <td>176.916672</td>\n      <td>0.004720</td>\n      <td>0.052083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.000400</td>\n      <td>0.010000</td>\n      <td>0.095161</td>\n      <td>162.750000</td>\n      <td>0.010088</td>\n      <td>0.010000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.000300</td>\n      <td>-0.005750</td>\n      <td>0.148576</td>\n      <td>183.333344</td>\n      <td>0.007003</td>\n      <td>-0.005750</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.000300</td>\n      <td>0.097500</td>\n      <td>0.087466</td>\n      <td>177.583344</td>\n      <td>0.007640</td>\n      <td>0.097500</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.000300</td>\n      <td>0.072917</td>\n      <td>0.057790</td>\n      <td>172.666672</td>\n      <td>0.007838</td>\n      <td>0.072917</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.000300</td>\n      <td>0.097583</td>\n      <td>0.067157</td>\n      <td>197.500000</td>\n      <td>0.007243</td>\n      <td>0.097583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.000500</td>\n      <td>0.453250</td>\n      <td>0.650133</td>\n      <td>159.666672</td>\n      <td>0.012259</td>\n      <td>0.036583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.083333</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.000600</td>\n      <td>0.072917</td>\n      <td>0.025516</td>\n      <td>151.833344</td>\n      <td>0.015932</td>\n      <td>0.072917</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.000400</td>\n      <td>0.055000</td>\n      <td>0.148148</td>\n      <td>183.000000</td>\n      <td>0.011098</td>\n      <td>0.055000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.000400</td>\n      <td>0.020833</td>\n      <td>0.032275</td>\n      <td>145.750000</td>\n      <td>0.009720</td>\n      <td>0.020833</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.000800</td>\n      <td>0.047333</td>\n      <td>0.120991</td>\n      <td>158.416672</td>\n      <td>0.018907</td>\n      <td>0.047333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.000300</td>\n      <td>0.139250</td>\n      <td>0.085936</td>\n      <td>185.250000</td>\n      <td>0.006964</td>\n      <td>0.139250</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.001200</td>\n      <td>0.081083</td>\n      <td>0.119742</td>\n      <td>162.583344</td>\n      <td>0.030971</td>\n      <td>0.081083</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.000500</td>\n      <td>0.043583</td>\n      <td>0.158672</td>\n      <td>146.333344</td>\n      <td>0.013024</td>\n      <td>0.043583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.000500</td>\n      <td>0.083333</td>\n      <td>0.059748</td>\n      <td>182.416672</td>\n      <td>0.013402</td>\n      <td>0.083333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>0.000500</td>\n      <td>0.080000</td>\n      <td>0.110227</td>\n      <td>190.833344</td>\n      <td>0.011554</td>\n      <td>0.080000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>0.000400</td>\n      <td>0.093750</td>\n      <td>0.057790</td>\n      <td>187.416672</td>\n      <td>0.010927</td>\n      <td>0.093750</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.000200</td>\n      <td>0.041583</td>\n      <td>0.098643</td>\n      <td>200.000000</td>\n      <td>0.005983</td>\n      <td>0.041583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.000800</td>\n      <td>-0.001167</td>\n      <td>0.280070</td>\n      <td>197.416672</td>\n      <td>0.019665</td>\n      <td>-0.001167</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.000700</td>\n      <td>0.290917</td>\n      <td>0.498441</td>\n      <td>186.666672</td>\n      <td>0.016907</td>\n      <td>0.082583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>0.000500</td>\n      <td>0.018333</td>\n      <td>0.253711</td>\n      <td>179.833344</td>\n      <td>0.011589</td>\n      <td>0.018333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>0.000600</td>\n      <td>0.274333</td>\n      <td>0.524533</td>\n      <td>188.750000</td>\n      <td>0.014459</td>\n      <td>0.066000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.041667</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>0.000800</td>\n      <td>0.093750</td>\n      <td>0.057790</td>\n      <td>197.083344</td>\n      <td>0.020290</td>\n      <td>0.093750</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"saving_name = 'LightFury9/' + hp_run_name\ntrainer.push_to_hub( saving_name , token='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')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}